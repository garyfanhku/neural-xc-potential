# Kohn-Sham Equations as Regularizer: Building Prior Knowledge
into Machine-Learned Physics

Authors: Li Li et al
Code: https://github.com/google-research/google-research/tree/master/jax_dft
Is-Survey: No
Library: Jax
Model: Global Convolution
URL: https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.126.036401
https://journals.aps.org/prl/supplemental/10.1103/PhysRevLett.126.036401/supp.pdf
Year: 2020


For XC, recent works focus on learning the XC potential
(not functional) from inverse KS [31] and use it in the
KS-DFT scheme [32–35]. An important step forward was
made last year, when it was shown that a neural network
could find functionals using only three molecules by
training on both energies and densities [36] [[Completing-dft-by-ml]], obtaining
accuracy comparable to human-designed functionals and
generalizing to yield accurate atomization energies of 148
small molecules [37]. But this pioneering work does not
yield chemical accuracy or approximations that work in the
dissociation limit. Moreover, it uses gradient-free optimization which usually suffers from poor convergence behavior on the large number of parameters used in modern
neural networks [38–40].


In other ML work, functionals
are trained on either energies alone [41–44], or even
densities [33,34 [[Toward-the-Exact-Exchange–Correlation-Potential]],45], but only after convergence. By incorporating the KS equations into the training, thereby
learning the relation between density and energy at every iteration, we find accurate models with very little data and
much greater generalizability.

Unlike previous works [33, 34 [[Toward-the-Exact-Exchange–Correlation-Potential]], 35 [[Neural-network-Kohn-Sham-exchange-correlation-potetial]] ] that explicitly
included the KS or XC potential in the loss function, our
model never uses the exact KS potential. In our KSR setup,
the model aims to predict ϵXC, from which the derived vs
yields accurate density. Therefore, predicting vXC is a
side product. We also address some concerns on training
explicitly with vXC. One artifact is that generating the exact
vs requires an additional inverse calculation, which is
known to be numerically unstable [31]. Schmidt et al.
[33] observe outliers while generating training vXC from
inverse KS. While vXC is a fascinating and useful object
for theoretical study because its relation to the density is
extremely delicate, it is far more practical to simply use the
density to train on [36] [[Completing-dft-by-ml]].

[//begin]: # "Autogenerated link references for markdown compatibility"
[Toward-the-Exact-Exchange–Correlation-Potential]: Toward-the-Exact-Exchange–Correlation-Potential.md "Toward the Exact Exchange–Correlation Potential: A Three-Dimensional Convolutional Neural Network Construct"
[Toward-the-Exact-Exchange–Correlation-Potential]: Toward-the-Exact-Exchange–Correlation-Potential.md "Toward the Exact Exchange–Correlation Potential: A Three-Dimensional Convolutional Neural Network Construct"
[Neural-network-Kohn-Sham-exchange-correlation-potetial]: Neural-network-Kohn-Sham-exchange-correlation-potetial.md "Neural-network Kohn-Sham exchange-correlation potential and its out-of-training transferability"
[//end]: # "Autogenerated link references"